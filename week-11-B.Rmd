---
title: "Week 11, Day 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(PPBDS.data)
library(rstanarm)
library(tidyverse)
library(tidymodels)

# The full shaming data is huge. We will learn more about how to work with such
# large data sets next semester in Gov 1005: Big Data. Join us! For now, let's
# sample 10,000 rows and work with that. Next Tuesday, we will use the full
# data set. In the meantime, feel free to experiment.

set.seed(1005)
week_11 <- shaming %>% 
  mutate(age = 2006 - birth_year) %>% 
  mutate(treatment = fct_relevel(treatment, "Control")) %>% 
  mutate(solo = ifelse(hh_size == 1, TRUE, FALSE)) %>% 
  select(-general_04, -no_of_names, -birth_year, -hh_size) %>% 
  sample_n(10000)

week_11_split <- initial_split(week_11)
week_11_train <- training(week_11_split)
week_11_test  <- testing(week_11_split)
week_11_folds <- vfold_cv(week_11_train, v = 5)
```


## Scene 1

**Prompt:** Explore a variety models which explain `primary_06` as a function of the variables in our data set. Make sure to explore some interaction terms. 

* Come up with at least two models that a) you like and would be willing to defend and b) are somewhat different from one another. The two most common model types in these situations are "simple" and "full". The former includes a minimum number of variables. The latter errs on the side of variable inclusion and the creation of interaction terms.

```{r sc1}

set.seed(10)

simple_wflow <- workflow() %>%
  add_recipe(recipe(primary_06 ~ treatment,
                    data = week_11_train) %>%
             step_dummy(all_nominal())) %>%
  add_model(linear_reg() %>% set_engine("stan"))

simple_wflow %>% 
  fit(data = week_11_train) %>% # Fit the model on training data
  predict(new_data = week_11_test) %>% # Test the model on testing data.
  bind_cols(week_11_test %>% select(primary_06)) %>% 
  metrics(truth = primary_06, estimate = `.pred`)

full_wflow <- workflow() %>%
  
  # I only use primary_02 instead of both primary_02 and primary_04 since '02
  # was a midterm primary.
  
  add_recipe(recipe(primary_06 ~ treatment + primary_02 + age + sex + solo,
                    data = week_11_train) %>%
             step_dummy(all_nominal()) %>%
             step_interact(~ starts_with("treatment"):starts_with("sex"))) %>%
  add_model(linear_reg() %>% set_engine("stan"))

full_wflow %>% 
  fit(data = week_11_train) %>% # Fit the model on training data
  predict(new_data = week_11_test) %>% # Test the model on testing data.
  bind_cols(week_11_test %>% select(primary_06)) %>% 
  metrics(truth = primary_06, estimate = `.pred`)

```

* Which data set should we use for this? Why?

The week_11_training dataset because we don't want to over-fit our data by 
including the testing dataset.

* What does it mean if, for example, the coefficient of `treatmentNeighbors` 
varies across models?

This means the Neighbors treatment in the first model might be unwittingly
including the effect of another variable and overestimating the effect of the
Neighbors treatment.

* Do things change if we start using all the data? Is there a danger in doing so?

If we use all the data, there's danger of over-fitting.


## Scene 2

**Prompt:** Compare your two models using cross-validation.

```{r sc2}

simple_wflow %>%
  fit_resamples(resamples = week_11_folds) %>%
  collect_metrics()

full_wflow %>%
  fit_resamples(resamples = week_11_folds) %>%
  collect_metrics()

```
The average RMSE value for the full model is slightly lower than the simple model.

## Scene 3

**Prompt:** Fit the model and then estimate what RMSE will be in the future.

* If you have time, redo all the important steps above with the full data set.

```{r sc3}

full_wflow %>% 
  fit(data = week_11) %>% # Fit the model on all the data
  predict(new_data = week_11_test) %>% # Test the model on testing data.
  bind_cols(week_11_test %>% select(primary_06)) %>% 
  metrics(truth = primary_06, estimate = `.pred`)
  
```

## Optional Problems

Challenge groups should be encouraged to make some plots. Hard thing about these plots is that the outcomes are all 0/1. Makes plotting much more of a challenge! Examples:

* Plot the primary_06 versus age for all the data. There are many ways to do that. Here is mine.

```{r sc4}

week_11 %>%
  group_by(age, primary_06) %>%
  summarize(primary_06_yes = n()) %>%
  filter(primary_06 == 1) %>%
  ggplot(aes(x = age, y = primary_06_yes)) +
    geom_col(color = "white") +
    labs(title = "Distribution of voter turnout by age in 2006 primary",
         x = "Age",
         y = "Primary '06 Voter Turnout")

```
* Plot the predicted values for the simple model versus the predicted values for the full model. How different are they?

* Plot the predicted values for the full model (fitted with all the training data) against the true values? Is there anything strange? Are there categories of observations with big residuals? Looking for such things can provide clues about how to improve the model.

* Do the same plots but with all 340,000 rows. What changes do we need to make the plots look good?



